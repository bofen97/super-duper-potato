{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parl\n",
    "from paddle import fluid\n",
    "from parl.core.fluid.policy_distribution import CategoricalDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariModel(parl.Model):\n",
    "    \"\"\"\n",
    "    only use for atari\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,actdim):\n",
    "        \n",
    "        self.conv1 = parl.layers.conv2d(num_filters=32,filter_size=8,stride=4,padding=1,act='relu')\n",
    "        self.conv2 = parl.layers.conv2d(num_filters=64,filter_size=4,stride=2,padding=2,act='relu')\n",
    "        self.conv3 = parl.layers.conv2d(num_filters=64,filter_size=3,stride=1,padding=0,act='relu')\n",
    "        self.fc = parl.layers.fc(size=512,act='relu')\n",
    "        \n",
    "        self.policy_fc = parl.layers.fc(size=actdim,act=None)\n",
    "        self.value_fc  = parl.layers.fc(size=1,act=None)\n",
    "        \n",
    "    def policy(self,obs):\n",
    "        obs = obs / 255.0\n",
    "        \n",
    "        conv1 = self.conv1(obs)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        \n",
    "        flatten = fluid.layers.flatten(conv3,axis=1)\n",
    "        \n",
    "        fc_output = self.fc(flatten)\n",
    "        \n",
    "        policy_logits = self.policy_fc(fc_output)\n",
    "        return policy_logits\n",
    "    def value(self,obs):\n",
    "        obs = obs / 255.0\n",
    "        \n",
    "        conv1 = self.conv1(obs)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        \n",
    "        flatten = fluid.layers.flatten(conv3,axis=1)\n",
    "        \n",
    "        fc_output = self.fc(flatten)\n",
    "        \n",
    "        values = self.value_fc(fc_output)\n",
    "        values = fluid.layers.squeeze(values,axes=[1])\n",
    "        return values\n",
    "    def policy_and_value(self,obs):\n",
    "        \n",
    "        \"\"\"\n",
    "        alg sample use it .\n",
    "        \n",
    "        INPUT : [ B,OBSERVATION_SPACE]\n",
    "        OUTPUT: [BATCHSIZE ACTDIM], [BATCHSIZE]\n",
    "        \"\"\"\n",
    "        obs = obs / 255.0\n",
    "        \n",
    "        conv1 = self.conv1(obs)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        \n",
    "        flatten = fluid.layers.flatten(conv3,axis=1)\n",
    "        \n",
    "        fc_output = self.fc(flatten)\n",
    "        \n",
    "        policy_logits = self.policy_fc(fc_output)\n",
    "        values = self.value_fc(fc_output)\n",
    "        # squeeze ..\n",
    "        values = fluid.layers.squeeze(values,axes=[1])\n",
    "        return policy_logits,values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class a2c(parl.Algorithm):\n",
    "    def __init__(self,model,config):\n",
    "        \n",
    "        self.model = model\n",
    "        self.vf_coeff = config['vf_coeff']\n",
    "    def value(self,obs):\n",
    "        value = self.model.value(obs)\n",
    "        return value\n",
    "    def predict(self,obs):\n",
    "        logits= self.model.policy(obs)\n",
    "        probs = fluid.layers.softmax(logits,axis=1)\n",
    "        predict_acts = fluid.layers.argmax(probs,axis=1)\n",
    "        return predict_acts\n",
    "        \n",
    "        \n",
    "    def sample(self,obs):\n",
    "        logits ,value= self.model.policy_and_value(obs)\n",
    "        probs = fluid.layers.softmax(logits,axis=1)\n",
    "        sample_acts = fluid.layers.sampling_id(probs)\n",
    "        return sample_acts,value\n",
    "    def learn(self,obs,act,adv,vtag,lr,ent_coeff):\n",
    "        logits =self.model.policy(obs)\n",
    "        \n",
    "        policy_distributions = CategoricalDistribution(logits)\n",
    "        \n",
    "        action_log_probs = policy_distributions.logp(act)\n",
    "        \n",
    "        pi_loss = -1.0  * fluid.layers.reduce_sum(action_log_probs * adv)\n",
    "        \n",
    "        values = self.model.value(obs)\n",
    "        \n",
    "        delta = values - vtag\n",
    "        vloss = 0.5*fluid.layers.reduce_sum(fluid.layers.square(delta))\n",
    "        \n",
    "        policy_entropy = policy_distributions.entropy()\n",
    "        \n",
    "        entropy = fluid.layers.reduce_sum(policy_entropy)\n",
    "        \n",
    "        total_loss = (pi_loss + vloss * self.vf_coeff + ent_coeff * entropy )\n",
    "        \n",
    "        fluid.clip.set_gradient_clip(\n",
    "            clip=fluid.clip.GradientClipByGlobalNorm(clip_norm=40.0))\n",
    "\n",
    "        optimizer = fluid.optimizer.AdamOptimizer(lr)\n",
    "        optimizer.minimize(total_loss)\n",
    "        \n",
    "        return total_loss,pi_loss,vloss,entropy\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-5614b25ca2d8>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-5614b25ca2d8>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    feed =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from parl.utils.scheduler import PiecewiseScheduler,LinearDecayScheduler\n",
    "\n",
    "class Agent(parl.Agent):\n",
    "    \n",
    "    def __init__(self,alg,config):\n",
    "        self.obs_shape = config['obs_shape']\n",
    "        \n",
    "        super(Agent, self).__init__(alg)\n",
    "\n",
    "        self.lr_scheduler = LinearDecayScheduler(config['start_lr'],\n",
    "                                                 config['max_sample_steps'])\n",
    "\n",
    "        self.entropy_coeff_scheduler = PiecewiseScheduler(\n",
    "            config['entropy_coeff_scheduler'])\n",
    "    def sample(self,obs):\n",
    "        \n",
    "        feed = \n",
    "        \n",
    "        \n",
    "    def build_program(self):\n",
    "        \n",
    "        self.sample_program = fluid.Program()\n",
    "        self.predict_program = fluid.Program()\n",
    "        self.value_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.sample_program):\n",
    "            obs = fluid.layers.data('obs',self.obs_shape,'float32')\n",
    "            self.sample_actions, self.sample_values = self.algorithm.sample(obs)\n",
    "            \n",
    "        with fluid.program_guard(self.predict_program):\n",
    "            obs = fluid.layers.data('obs',self.obs_shape,'float32')\n",
    "            self.predict_acts = self.algorithm.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.value_program):\n",
    "            obs = fluid.layers.data('obs',self.obs_shape,'float32')\n",
    "            self.values = self.algorithm.value(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = fluid.layers.data('obs',self.obs_shape,'float32')\n",
    "            act = fluid.layers.data('act',[],'int64')\n",
    "            adv = fluid.layers.data('adv',[],'float32')\n",
    "            vtag = fluid.layers.data('vtag',[],'float32')\n",
    "            \n",
    "            lr = fluid.layers.data(\n",
    "                name='lr', shape=[1], dtype='float32', append_batch_size=False)\n",
    "            entropy_coeff = fluid.layers.data(\n",
    "                name='entropy_coeff', shape=[], dtype='float32')\n",
    "\n",
    "            total_loss, pi_loss, vf_loss, entropy = self.algorithm.learn(\n",
    "                obs, act, adv, vtag, lr, entropy_coeff)\n",
    "            self.learn_outputs = [total_loss, pi_loss, vf_loss, entropy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "config ={}\n",
    "config['vf_coeff'] = 0.1\n",
    "config['start_lr'] = 1e-3\n",
    "config['max_sample_steps'] = 100\n",
    "config['entropy_coeff_scheduler'] =([0 , -0.01])\n",
    "\n",
    "obs = fluid.layers.data('obs',[4,80,80],'float32')\n",
    "act = fluid.layers.data('act',[],'int64')\n",
    "adv = fluid.layers.data('adv',[],'float32')\n",
    "vtag= fluid.layers.data('vtag',[],'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AtariModel(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = a2c(model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(name: \"tmp_61\"\n",
       " type {\n",
       "   type: LOD_TENSOR\n",
       "   lod_tensor {\n",
       "     tensor {\n",
       "       data_type: FP32\n",
       "       dims: 1\n",
       "     }\n",
       "     lod_level: 0\n",
       "   }\n",
       " }, name: \"tmp_44\"\n",
       " type {\n",
       "   type: LOD_TENSOR\n",
       "   lod_tensor {\n",
       "     tensor {\n",
       "       data_type: FP32\n",
       "       dims: 1\n",
       "     }\n",
       "     lod_level: 0\n",
       "   }\n",
       " }, name: \"tmp_49\"\n",
       " type {\n",
       "   type: LOD_TENSOR\n",
       "   lod_tensor {\n",
       "     tensor {\n",
       "       data_type: FP32\n",
       "       dims: 1\n",
       "     }\n",
       "     lod_level: 0\n",
       "   }\n",
       " }, name: \"reduce_sum_25.tmp_0\"\n",
       " type {\n",
       "   type: LOD_TENSOR\n",
       "   lod_tensor {\n",
       "     tensor {\n",
       "       data_type: FP32\n",
       "       dims: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " persistable: false)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.learn(obs,act,adv,vtag,0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-atari] *",
   "language": "python",
   "name": "conda-env-.conda-atari-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
